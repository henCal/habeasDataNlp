{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "import glob\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "####github.com/michmech\n",
    "from pt_lematizador import PortugueseMess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes all .docx files in a folder, and writes .txt files with the same content and name as each found.\n",
    "#the input are the files to be turned into .txt files, an the path to find them, eg. path = r'C:\\Documents\\Docx_Files\\*.docx'\n",
    "##it is important that the end of the path is *.docx\n",
    "##it is important that either you make every \\ a \\\\ or add a r before the string\n",
    "#the output is a .txt file for each .docx file found in the folder\n",
    "\n",
    "\n",
    "def docx_to_txt(path):\n",
    "    directory = glob.glob(path)\n",
    "    for file_name in directory:\n",
    "        with open(file_name, 'rb') as infile:\n",
    "            outfile = open(file_name[:-5] + '.txt', 'w', encoding='utf-8')\n",
    "            doc = docx2txt.process(infile)\n",
    "            outfile.write(doc)\n",
    "            infile.close()\n",
    "            outfile.close()\n",
    "        \n",
    "\n",
    "    return \"txt files done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes all .txt files in a folder, and writes all their content in a new file\n",
    "#the input are the files to be compiled and the path to find them, eg. path = r'C:\\Documents\\Docx_Files\\*.txt'\n",
    "##it is important that the end of the path is *.txt\n",
    "##it is important that either you make every \\ a \\\\ or add a r before the string\n",
    "#the output is a file named 'all_files.txt' with all text in the files found in the folder, separated by a \\n\n",
    "\n",
    "def compile_txt(path):\n",
    "    directory = glob.glob(path)\n",
    "    corpus = \" \"\n",
    "    \n",
    "    for file_name in directory:\n",
    "        with open(file_name, 'r', encoding = 'utf-8') as file:\n",
    "            corpus += file.read()\n",
    "        file.close()\n",
    "        corpus += '\\n'\n",
    "            \n",
    "    with open('all_files.txt', 'w', encoding = 'utf-8') as newfile:\n",
    "        newfile.write(corpus)\n",
    "        newfile.close()\n",
    "    \n",
    "    return \"compiled txt file done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns a list of strings to be used as stopwords\n",
    "#the function takes the portuguese stopwords from nltk and adds to it from two .txt files, nomes.txt and outras.txt\n",
    "#this was done so that the user could easily add to the list of stopwords two different types of words that were reccuring in his project,\n",
    "#names of people and terms common to the documents that did not bring any useful insights, even though in a different context possibly could,\n",
    "#thus are not traditionally considered to be stopwords.\n",
    "\n",
    "def stopwords_fun():\n",
    "   \n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "    with open(\"outras.txt\",\"r\", encoding = \"utf-8\") as file:\n",
    "        outras = file.read()\n",
    "        file.close()\n",
    "    stopwords = stopwords + nltk.word_tokenize(outras)\n",
    "\n",
    "    with open(\"nomes.txt\",\"r\", encoding = \"utf-8\") as file:\n",
    "        nomes = file.read()\n",
    "        file.close()\n",
    "    stopwords = stopwords + nltk.word_tokenize(nomes)\n",
    "\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function returns a list of strings considered important tokens\n",
    "#the input is a string (text) and a list of strings (stopwords)\n",
    "#the function tokenizes the text, removes any token that is not alpha, removes any token found in the stopwords list, and returns the resulting list of strings\n",
    "\n",
    "def content_tokenize(text, stopwords):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens_alpha = [w.lower() for w in tokens if w.isalpha()]\n",
    "    important_tokens = [w for w in tokens_alpha if w not in stopwords and len(w) > 2 and w.isalpha()]\n",
    "    return important_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a filename for a txt file, and returns its contents as a string\n",
    "#because this can be used often, making it a function makes the code easier to read\n",
    "\n",
    "def read_file(filename):   \n",
    "    with open(filename, 'r', encoding = 'utf-8') as file:\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns a list of strings with the names of each file in a folder\n",
    "\n",
    "\n",
    "def filename_list(path):\n",
    "    directory = glob.glob(path)\n",
    "    filenames = []\n",
    "    for filename in directory:\n",
    "        filenames.append(filename)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a list of tuples and returns a list of tuples\n",
    "#the output is the same list as the input, but lowering the strings within the tuples and removing any duplicates\n",
    "\n",
    "def ngram_set_fun(ngram_list):\n",
    "    ngram_set = []\n",
    "    for ngram in ngram_list:\n",
    "        if ngram not in ngram_set:\n",
    "            ngram_set.append(ngram)\n",
    "    return ngram_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function makes a list of tuples containing the ngrams from every file in a path without repetition in each set\n",
    "#n == 2 means bigrams, n == 3 means trigrams, etc.\n",
    "\n",
    "def list_sets_fun(path, n = 1):\n",
    "   list_sets = []\n",
    "   filenames = filename_list(path)\n",
    "   for filename in filenames:\n",
    "      with open(filename, 'r', encoding = 'utf-8') as file:\n",
    "         #file_ngram_set = ngram_set_fun(list(nltk.ngrams(content_tokenize(file.read(),stopwords_fun()), n)))\n",
    "         file_ngram_set = ngram_set_fun(nltk.ngrams(nltk.word_tokenize(file.read()), n))\n",
    "         file.close()\n",
    "         list_sets.append(list(file_ngram_set[:]))\n",
    "   \n",
    "   for set in list_sets:\n",
    "      if len(set) == 0:\n",
    "         list_sets.remove(set)\n",
    "   return list_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a list of the tuples found in each set, and the name of the output file as a string\n",
    "#the result is a coocurrence matrix csv file, separated by ';' instead of the usual ','. that can be changed in line XX of this cell\n",
    "\n",
    "def cooccurrence_csv(each_set, output_filename):\n",
    "    complete_set = []\n",
    "    for set in each_set:\n",
    "        for token in set:\n",
    "            if token not in complete_set:\n",
    "                complete_set.append(token)\n",
    "    \n",
    "    cooccurrence = {}\n",
    "\n",
    "    for ngram in complete_set:\n",
    "        cooccurrence[ngram] = {}\n",
    "\n",
    "    for text_ngrams in each_set:\n",
    "        for ngram in text_ngrams:\n",
    "            for other_ngram in text_ngrams:\n",
    "                if ngram != other_ngram:\n",
    "                    if other_ngram not in cooccurrence[ngram]:\n",
    "                        cooccurrence[ngram][other_ngram] = 0\n",
    "                    cooccurrence[ngram][other_ngram] += 1\n",
    "\n",
    "    with open(output_filename, 'w', newline='',) as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = ';')\n",
    "        header = [''] + [str(ngram) for ngram in complete_set]\n",
    "        writer.writerow(header)\n",
    "        for ngram in complete_set:\n",
    "            row = [str(ngram)]\n",
    "            for other_ngram in complete_set:\n",
    "                count = cooccurrence[ngram].get(other_ngram, 0)\n",
    "                row.append(count)\n",
    "            writer.writerow(row)\n",
    "        csvfile.close()\n",
    "\n",
    "    return \"coocurrence file done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function has the purpose of looking for ngrams that start with any term in a given list of terms\n",
    "#the input is two lists of strings and an int value n, that determines the lenght of the ngram\n",
    "#the output is a list of strings\n",
    "#the search is not case sensitive\n",
    "\n",
    "\n",
    "def search_ngrams(token_list, search_term_list, n = 1):\n",
    "    search_results = []\n",
    "    ngrams = list(nltk.ngrams(token_list, n))\n",
    "    for term in search_term_list:\n",
    "        for ngram in ngrams:\n",
    "            if term.lower() == ngram[0].lower():\n",
    "                search_results.append(ngram)\n",
    "    \n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a list of tuples of strings, and returns a .txt file containing its contents\n",
    "#the strings are separated by spaces, and the tuples by lines\n",
    "#example of output_filename: 'friends_list.txt'\n",
    "\n",
    "def tuple_list_to_txt(tuple_list, output_filename):\n",
    "    file_content = ''\n",
    "    for tup in tuple_list:\n",
    "        for word in tup:\n",
    "            file_content += word\n",
    "            file_content += ' '\n",
    "        file_content += ('\\n')\n",
    "\n",
    "    with open(output_filename, 'w', encoding = 'utf-8') as file:\n",
    "        file.write(file_content)\n",
    "        file.close()\n",
    "\n",
    "    return \"txt file done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function makes a unigram tagger that, when meeting an unknown word, assumes it is a noun\n",
    "#this was done because usually it would fail to identify proper nouns\n",
    "#takes no parameters\n",
    "#should be saved in a variable, like so: tagger = tagger_maker()\n",
    "#and used like so: tagged_tokens_list = tagger.tag(untagged_token_list)\n",
    "#this procedure results in a list of tuples that looks like this: [('token1', 'N')]\n",
    "\n",
    "def tagger_maker():\n",
    "    train = nltk.corpus.mac_morpho.tagged_sents()\n",
    "    tagger = nltk.tag.UnigramTagger(train, backoff = nltk.tag.DefaultTagger('N'))\n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function makes a list of tuples with the token and it's tag\n",
    "#its parameters are the list of strings of tokens the user wants to tag, and a nltk.tag.sequential.UnigramTagger\n",
    "#by default, the tagger will be the one obtained from the function tagger_maker()\n",
    "\n",
    "def token_tagger(token_list, tagger = tagger_maker()):\n",
    "    return tagger.tag(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function asks the user to type .txt or .docx, and returns a string containing the anwser\n",
    "\n",
    "def interface_type_input(type_options = ['.txt', '.docx']):\n",
    "    type_input = str(input('Digite o tipo dos arquivos a serem analisados (.txt ou .docx):\\n')).lower().strip()\n",
    "    if type_input[0] != '.':\n",
    "        type_input = '.' + type_input\n",
    "    if type_input not in type_options:\n",
    "        print('O arquivo deve ser do tipo .txt ou .docx. Certifique-se de que o valor digitado está correto.\\n')\n",
    "        return interface_type_input(type_options)\n",
    "    return type_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function asks the user for the path of the files in the computer\n",
    "#to avoid bugs and errors, every \\ and every / is replaced by \\\\\n",
    "#returns a string with the path\n",
    "\n",
    "def interface_path_input():\n",
    "    path_input = str(input('Digite o caminho para a pasta onde se encontram os arquivos (exemplo: C:\\Documents\\Analise\\Arquivos\\):\\n'))\n",
    "    if '\\\\\\\\' not in path_input:\n",
    "        if '/'in path_input:\n",
    "            path_input = path_input.replace('/', '\\\\')\n",
    "        path_input = path_input.replace('\\\\', '\\\\\\\\')\n",
    "    if path_input[-1] != '\\\\':\n",
    "        path_input += '\\\\\\\\'\n",
    "    return path_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function asks the user what functions they would like to use through yes or no questions\n",
    "#returns a list of subslists containg a string with the question asked and a bool var with anwser given\n",
    "\n",
    "def interface_user_picks():\n",
    "    print('''Decida quais operações o programa deve realizar:\\n\n",
    "          Caso se arrependa de qualquer decisão realizada, digite \"Recomeçar\".\\n\n",
    "          Responda as perguntas digitando \"Sim\" ou \"Não\".\\n\\n''')\n",
    "    positive_terms = ['sim', 's', 'si', 'im', 'yes', 'y']\n",
    "    negative_terms = ['não', 'nao', 'nã', 'na','ão', 'ao', 'no', 'n']\n",
    "    decisions = [['''Remover Stopwords?\\n\n",
    "                  (Stopwords do pacote nltk, termos não alfabéticos, termos com menos que duas letras e aqueles encontradas nos arquivos nomes.txt e outras.txt)\\n'''],\n",
    "                 ['Tornar tokens stems? (Não recomendado caso deseje utilizar a função search_ngrams)\\n'],\n",
    "                 ['''Deseja utilizar a função search_ngrams?\\n\n",
    "                  (O usuário define um ou mais termos a serem buscados, e serão exibidas as ocorrências desses termos com os tokens\n",
    "                  encontrados em sequência, facilitando leitura do contexto onde os termos escolhidos se inserem).\\n'''],\n",
    "                  ['Deseja gerar um arquivo csv de coocorrência de tokens/ngrams? (Verifica quais tokens ou ngrams aparecem conjuntamente nos arquivos)\\n'],\n",
    "                  ['Deseja visualizar os tokens/ngrams mais frequentes?\\n']]\n",
    "\n",
    "\n",
    "    for index in range(len(decisions)):\n",
    "        question = decisions[index][0]\n",
    "        anwser = input(decisions[index][0]).lower().strip()\n",
    "\n",
    "        if anwser not in positive_terms and anwser not in negative_terms:\n",
    "            return interface_user_picks()\n",
    "        if anwser in positive_terms:\n",
    "            decisions[index].append(True)\n",
    "        else:\n",
    "            decisions[index].append(False)\n",
    "    print('Fim decisões.\\n\\n')\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function asks what words the user would like to search within the list_of_tokens, and how many tokens after\n",
    "#returns a list of the sublist containing each term searched, the result from calling the function search_ngrams with the given parameters\n",
    "\n",
    "def interface_user_searches(list_of_tokens):\n",
    "    tokens_after_term = int(input('Digite, em algarismos, quantas palavras deseja visualizar após cada termo buscado que for encontrado:\\n')) + 1\n",
    "    print('''Digite os termos que deseja buscar, pressionando Enter depois de cada um. Quando terminar, digite 000.''')\n",
    "    list_of_terms = []\n",
    "    term = ''\n",
    "    while term != '000':\n",
    "        term = str(input()).strip()\n",
    "        list_of_terms.append(term.lower())\n",
    "\n",
    "    return [list_of_terms[:-1], search_ngrams(list_of_tokens, list_of_terms, tokens_after_term)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function asks the user how many of the most common bigrams in the list_of_tokens they would like to see, and the value n\n",
    "#returns a list with the resulting FreqDist, and the amount choosen\n",
    "\n",
    "def interface_freqdist(list_of_tokens):\n",
    "    amount = int(input('''Digite, em algarismos, o número de ngrams mais frequentes a serem visualizados: '''))\n",
    "    n = int(input('Digite, em algarismos, o valor n dos ngrams a serem observados (1 para tokens, 2 para bigrams, 3 para trigrams, etc): '))\n",
    "    \n",
    "    return [nltk.FreqDist(nltk.ngrams(list_of_tokens, n)), amount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function facilitates the use of the cooccurence_csv function\n",
    "#its parameters are the path to the files as a string, and a bool stating wheter to remove stopwords or not\n",
    "#then, the function asks the user for the value n of the ngrams, and the name of the file to be generated\n",
    "#then the function calls for cooccurrence_csv with the given filename and the list of sets, with or without stopwords and non alphabetic values, depending on bool_stopwords\n",
    "\n",
    "def interface_coocurrence(path, bool_stopwords):\n",
    "    n_input = int(input(\n",
    "        '''Digite, em algarismo, o valor n do ngram que se deseja observar a coocorrência.\\n\n",
    "        (1 para token, 2 para bigrama, 3 para trigrama, etc.)\\n\n",
    "        (Quanto maior esse valor, mais lento será a geração do arquivo e maior será seu tamanho) '''))\n",
    "    \n",
    "\n",
    "    filename = input('Digite o nome do arquivo a ser gerado: ')\n",
    "    if filename[-4:] != '.csv':\n",
    "        filename += '.csv'\n",
    "    \n",
    "\n",
    "\n",
    "    if bool_stopwords:\n",
    "        remove_ngram = False\n",
    "        stopwords = stopwords_fun()\n",
    "        complete_set = list_sets_fun(path, n_input)\n",
    "        every_set = []\n",
    "        for set in complete_set:\n",
    "            every_set.append([])\n",
    "        #each set\n",
    "        for i in range(len(complete_set)):\n",
    "            #each ngram\n",
    "            for j in range(len(complete_set[i])):\n",
    "                append_ngram = True\n",
    "                for token in complete_set[i][j]:\n",
    "                    if token in stopwords or token.isalpha() == False:\n",
    "                        append_ngram = False\n",
    "                if append_ngram == True:\n",
    "                    every_set[i].append(complete_set[i][j])\n",
    "\n",
    "    else:\n",
    "        every_set = list_sets_fun(path, n_input)\n",
    "    \n",
    "\n",
    "    return cooccurrence_csv(every_set, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calls the interface_ functions, and runs them according to the anwsers given by the user in interface_user_picks\n",
    "\n",
    "def interface():\n",
    "    print('''Orientações:\n",
    "          \\nOs arquivos a serem analisados devem ser todos do mesmo tipo, .docx ou .txt.\n",
    "          \\nDevem todos se encontrar em uma mesma pasta em seu diretório.\n",
    "          \\nSerão analisados todos os arquivos da pasta indicada.\n",
    "          \\n\\nAVISOS:\n",
    "          \\nRodar esse programa irá gerar um arquivo denominado all_files.txt.\n",
    "          \\nRodar esse programa para ler arquivos .docx irá gerar arquivos .txt que levam o mesmo nome que seus equivalentes em .docx\n",
    "          \\nCaso seja encontrado arquivo com nome idêntico a um a ser gerado, o arquivo antigo será substituido.\n",
    "          \\nRecomenda-se que o programa seja utilizado por um usuário com conhecimentos de Python, e que o script seja adaptado para as\n",
    "necessidades específicas do usuário.\n",
    "          \\n\\n\n",
    "          ''')\n",
    "    \n",
    "    type_input = interface_type_input()\n",
    "    path_input = interface_path_input()\n",
    "\n",
    "    path = path_input + '*' + type_input\n",
    "    if type_input == '.docx':\n",
    "        docx_to_txt(path)\n",
    "    path = path_input + '*.txt'\n",
    "    \n",
    "    compile_txt(path)\n",
    "    all_text = read_file('all_files.txt')\n",
    "    all_tokens = nltk.word_tokenize(all_text)\n",
    "\n",
    "    user_picks = interface_user_picks()\n",
    "\n",
    "    #remove stopwords\n",
    "    if user_picks[0][1] == True:\n",
    "        all_content = content_tokenize(all_text, stopwords_fun())\n",
    "    else: \n",
    "        all_content = [word.lower() for word in all_tokens]\n",
    "    \n",
    "    #stem tokens\n",
    "    if user_picks[1][1] == True:\n",
    "        #for portuguese:\n",
    "        all_lemmas = [PortugueseMess(word) for word in all_content]\n",
    "        all_content = all_lemmas\n",
    "\n",
    "        #for english:\n",
    "        ##stemmer = nltk.stem.WordNetLemmatizer()\n",
    "        ##all_lemmas = [stemmer.lemmatize(word) for word in all_content]\n",
    "        ##all_content = all_lemmas\n",
    "\n",
    "    #coocurrence_csv\n",
    "    if user_picks[3][1] == True:\n",
    "        interface_coocurrence(path, user_picks[0][1])\n",
    "\n",
    "    #ngram_search\n",
    "    if user_picks[2][1] == True:\n",
    "        search_result = interface_user_searches(all_content)\n",
    "        print('\\nTermos buscados:')\n",
    "        for term in search_result[0]:\n",
    "            print('\\t- \"', term, '\"', sep = '')\n",
    "        print('\\nFrases encontradas:')\n",
    "        for result in search_result[1]:\n",
    "            print('\\t- \"', result, '\"', sep = '')\n",
    "           \n",
    "    #view freqdist graph of ngrams\n",
    "    if user_picks[4][1] == True:\n",
    "        ngram_freqdist = interface_freqdist(all_content)\n",
    "        print('\\nGráfico dos ngrams mais frequentes:')\n",
    "        ngram_freqdist[0].plot(ngram_freqdist[1], cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\henri\\Documents\\ADM\\HabeasData\\PLN\\MuitosReus\\docs\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
